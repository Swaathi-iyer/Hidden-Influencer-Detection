{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swaathi-iyer/Hidden-Influencer-Detection/blob/main/Hidden_Influencer_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Swaathi-iyer/Hidden-Influencer-Detection.git"
      ],
      "metadata": {
        "id": "l_ttaxQWROQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLD3PiCTw0Vv"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.26.4 -q\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n",
        "!pip install node2vec networkx pandas scikit-learn matplotlib seaborn tqdm -q\n",
        "\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    cuda_version = torch.version.cuda.replace('.', '')\n",
        "    print(f\"Detected CUDA Version: {cuda_version}\")\n",
        "    !pip install -q torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}+{cuda_version}.html\n",
        "    !pip install torch-geometric -q\n",
        "    print(\"PyTorch Geometric installed for GPU.\")\n",
        "else:\n",
        "    print(\"CUDA not available. Installing CPU version.\")\n",
        "    !pip install -q torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}+cpu.html\n",
        "    !pip install torch-geometric -q\n",
        "    print(\"PyTorch Geometric installed for CPU.\")\n",
        "\n",
        "print(\"Graph Dependencies check complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUUK0O8Ty1id"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "REQUIRED_FILES = [\n",
        "    'twitter_edges.csv',\n",
        "    'twitter_node_features.csv',\n",
        "    'twitter_graph.gpickle',\n",
        "    'twitter_metadata.csv',\n",
        "    'youtube_edges.csv',\n",
        "    'youtube_node_features.csv',\n",
        "    'youtube_graph.gpickle',\n",
        "    'youtube_metadata.csv'\n",
        "]\n",
        "\n",
        "print(\"Checking for required files...\")\n",
        "\n",
        "missing_files = [f for f in REQUIRED_FILES if not os.path.exists(f)]\n",
        "\n",
        "if missing_files:\n",
        "    print(\"\\nCRITICAL ERROR: The following required files are missing:\")\n",
        "    for f in missing_files:\n",
        "        print(f\"   - {f}\")\n",
        "    print(\"\\nPlease upload these files to Colab before running this cell.\")\n",
        "    raise FileNotFoundError(\"Missing required data files.\")\n",
        "else:\n",
        "    print(f\"All {len(REQUIRED_FILES)} required files are present.\")\n",
        "    print(\"Ready to proceed to next cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bQHVloZy6Gq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from node2vec import Node2Vec\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (classification_report, roc_auc_score,\n",
        "                             confusion_matrix, precision_recall_curve,\n",
        "                             average_precision_score)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"Initializing Environment & Checking Device\")\n",
        "print(f\"Using device: {device.type.upper()}\")\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(f\"Note: CUDA not available. Training on CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"LOADING DATASETS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def load_data_file(filename, file_type='csv'):\n",
        "    if not os.path.exists(filename):\n",
        "        raise FileNotFoundError(f\"CRITICAL ERROR: Required file '{filename}' not found.\")\n",
        "\n",
        "    if file_type == 'csv':\n",
        "        return pd.read_csv(filename, dtype={'source_user_id': object, 'target_user_id': object, 'user_id': object, 'user': object, 'channel_name': object})\n",
        "    elif file_type == 'metadata':\n",
        "        try:\n",
        "            with open(filename, 'r') as f:\n",
        "                data = json.load(f)\n",
        "            return data[0] if isinstance(data, list) else data\n",
        "        except Exception as e:\n",
        "            try:\n",
        "                return pd.read_csv(filename).to_dict('records')[0]\n",
        "            except Exception:\n",
        "                return {}\n",
        "\n",
        "print(f\"\\nLOADING TWITTER DATA...\")\n",
        "twitter_edges = load_data_file('twitter_edges.csv', 'csv')\n",
        "twitter_nodes = load_data_file('twitter_node_features.csv', 'csv')\n",
        "\n",
        "if 'user' in twitter_nodes.columns:\n",
        "    twitter_nodes.rename(columns={'user': 'user_id'}, inplace=True)\n",
        "elif 'user_id' not in twitter_nodes.columns:\n",
        "    raise KeyError(\"Missing user identifier column in twitter_node_features.\")\n",
        "\n",
        "twitter_meta = load_data_file('twitter_metadata.csv', 'metadata')\n",
        "print(f\"Edges: {len(twitter_edges):,}\")\n",
        "print(f\"Nodes: {len(twitter_nodes):,}\")\n",
        "\n",
        "print(f\"\\nLOADING YOUTUBE DATA...\")\n",
        "youtube_edges = load_data_file('youtube_edges.csv', 'csv')\n",
        "youtube_nodes = load_data_file('youtube_node_features.csv', 'csv')\n",
        "\n",
        "if 'channel_name' in youtube_nodes.columns:\n",
        "    youtube_nodes.rename(columns={'channel_name': 'user_id'}, inplace=True)\n",
        "elif 'user_id' not in youtube_nodes.columns:\n",
        "    raise KeyError(\"Missing user identifier column in youtube_node_features.\")\n",
        "\n",
        "youtube_meta = load_data_file('youtube_metadata.csv', 'metadata')\n",
        "print(f\"Edges: {len(youtube_edges):,}\")\n",
        "print(f\"Nodes: {len(youtube_nodes):,}\")\n",
        "\n",
        "if 'source_user_id' not in twitter_edges.columns and 'source' in twitter_edges.columns:\n",
        "    twitter_edges.rename(columns={'source': 'source_user_id', 'target': 'target_user_id'}, inplace=True)\n",
        "if 'source_user_id' not in youtube_edges.columns and 'source' in youtube_edges.columns:\n",
        "    youtube_edges.rename(columns={'source': 'source_user_id', 'target': 'target_user_id'}, inplace=True)\n",
        "\n",
        "all_edges = pd.concat([twitter_edges, youtube_edges], ignore_index=True)\n",
        "all_nodes = pd.concat([twitter_nodes, youtube_nodes], ignore_index=True)\n",
        "\n",
        "print(f\"\\nCOMBINED DATA SUMMARY:\")\n",
        "print(f\"Total unique users (nodes): {all_nodes['user_id'].nunique():,}\")\n",
        "print(f\"Total node features rows: {len(all_nodes):,}\")\n",
        "print(f\"Total edges rows: {len(all_edges):,}\")"
      ],
      "metadata": {
        "id": "u9TcHngmMqHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATA PREPROCESSING + LABEL GENERATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "all_nodes['followers'] = all_nodes['subscribers'].fillna(0) + \\\n",
        "                         (all_nodes['pagerank'] * 1000000 * np.random.uniform(0.8, 1.2)).fillna(0)\n",
        "all_nodes['followers'] = all_nodes['followers'].apply(lambda x: max(100, x)).astype(int)\n",
        "\n",
        "all_nodes['posts_count'] = (200 + all_nodes['pagerank'] * 100000 + np.random.randint(-50, 50)).astype(int)\n",
        "all_nodes['posts_count'] = all_nodes['posts_count'].apply(lambda x: max(1, x))\n",
        "\n",
        "all_nodes['engagement_rate'] = (\n",
        "    0.01 +\n",
        "    all_nodes['pagerank'] * 50 +\n",
        "    all_nodes['influence_score'].fillna(0) * 0.1 +\n",
        "    np.random.uniform(0.001, 0.05)\n",
        ")\n",
        "all_nodes['engagement_rate'] = all_nodes['engagement_rate'].clip(0.001, 0.5)\n",
        "\n",
        "all_nodes['mfim_score'] = (\n",
        "    all_nodes['influence_score'].fillna(0) * 0.8 +\n",
        "    all_nodes['pagerank'] * 50 +\n",
        "    np.random.uniform(0.1, 0.3)\n",
        ")\n",
        "all_nodes['mfim_score'] = all_nodes['mfim_score'].clip(0.0, 1.0)\n",
        "\n",
        "all_nodes['authenticity_score'] = np.random.uniform(0.8, 0.95, size=len(all_nodes))\n",
        "\n",
        "feature_cols = [col for col in all_nodes.columns\n",
        "                if col not in ['user_id', 'platform', 'subscribers', 'total_views', 'influence_score']]\n",
        "\n",
        "print(f\"\\nHandling missing values...\")\n",
        "all_nodes[feature_cols] = all_nodes[feature_cols].fillna(0)\n",
        "\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATING LABELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "engagement_threshold = all_nodes['engagement_rate'].quantile(0.50)\n",
        "follower_threshold = all_nodes['followers'].quantile(0.75)\n",
        "mfim_threshold = all_nodes['mfim_score'].quantile(0.40)\n",
        "authenticity_threshold = 0.8\n",
        "min_posts = 10\n",
        "\n",
        "all_nodes['is_hidden_influencer'] = (\n",
        "    (all_nodes['engagement_rate'] >= engagement_threshold) &\n",
        "    (all_nodes['followers'] <= follower_threshold) &\n",
        "    (all_nodes['mfim_score'] >= mfim_threshold) &\n",
        "    (all_nodes['authenticity_score'] > authenticity_threshold) &\n",
        "    (all_nodes['posts_count'] > min_posts)\n",
        ").astype(int)\n",
        "\n",
        "hidden_count = all_nodes['is_hidden_influencer'].sum()\n",
        "total_count = len(all_nodes)\n",
        "hidden_percentage = (hidden_count / total_count) * 100\n",
        "\n",
        "print(f\"\\nLABELS GENERATED\")\n",
        "print(f\"Total users: {total_count:,}\")\n",
        "print(f\"Hidden influencers: {hidden_count:,} ({hidden_percentage:.1f}%)\")\n",
        "print(f\"Regular users: {total_count - hidden_count:,} ({100-hidden_percentage:.1f}%)\")\n",
        "\n",
        "labels_df = all_nodes[['user_id', 'is_hidden_influencer']]\n",
        "labels_df.to_csv('labels_generated.csv', index=False)\n",
        "\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"NORMALIZING FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "feature_cols_for_norm = [col for col in feature_cols]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "all_nodes[feature_cols_for_norm] = scaler.fit_transform(all_nodes[feature_cols_for_norm])\n",
        "\n",
        "all_nodes = all_nodes.reset_index(drop=True)\n",
        "user_to_idx = {user_id: idx for idx, user_id in enumerate(all_nodes['user_id'])}\n",
        "idx_to_user = {idx: user_id for user_id, idx in user_to_idx.items()}\n",
        "\n",
        "print(f\"\\nPreprocessing complete\")\n",
        "print(f\"Total users: {len(all_nodes):,}\")\n",
        "print(f\"Feature dimension: {len(feature_cols_for_norm)}\")"
      ],
      "metadata": {
        "id": "9sqmgD9dMxZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING NODE2VEC EMBEDDINGS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "G = nx.Graph()\n",
        "for _, row in all_edges.iterrows():\n",
        "    src = row['source_user_id']\n",
        "    tgt = row['target_user_id']\n",
        "    weight = row.get('weight', 1.0)\n",
        "\n",
        "    if src in user_to_idx and tgt in user_to_idx:\n",
        "        G.add_edge(src, tgt, weight=weight)\n",
        "\n",
        "graph_nodes = set(G.nodes())\n",
        "valid_users = set(all_nodes['user_id'])\n",
        "nodes_to_remove = list(graph_nodes - valid_users)\n",
        "G.remove_nodes_from(nodes_to_remove)\n",
        "\n",
        "print(f\"\\nGraph statistics:\")\n",
        "print(f\"Nodes: {G.number_of_nodes():,}\")\n",
        "print(f\"Edges: {G.number_of_edges():,}\")\n",
        "print(f\"Density: {nx.density(G):.6f}\")\n",
        "\n",
        "if nx.is_connected(G):\n",
        "    print(f\"Graph is connected\")\n",
        "else:\n",
        "    components = list(nx.connected_components(G))\n",
        "    print(f\"Graph has {len(components)} components\")\n",
        "    if components:\n",
        "        print(f\"Largest component: {len(max(components, key=len)):,} nodes\")\n",
        "\n",
        "print(f\"\\nTraining Node2Vec...\")\n",
        "\n",
        "node2vec = Node2Vec(\n",
        "    G,\n",
        "    dimensions=128,\n",
        "    walk_length=30,\n",
        "    num_walks=200,\n",
        "    workers=4,\n",
        "    p=1,\n",
        "    q=1,\n",
        "    quiet=False\n",
        ")\n",
        "\n",
        "model_n2v = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
        "\n",
        "print(f\"\\nNode2Vec training complete\")\n",
        "\n",
        "print(f\"\\nExtracting embeddings...\")\n",
        "graph_embeddings = []\n",
        "for user_id in all_nodes['user_id']:\n",
        "    try:\n",
        "        emb = model_n2v.wv[user_id]\n",
        "    except KeyError:\n",
        "        emb = np.zeros(128)\n",
        "    graph_embeddings.append(emb)\n",
        "\n",
        "graph_embeddings = np.array(graph_embeddings)\n",
        "print(f\"Shape: {graph_embeddings.shape}\")\n",
        "\n",
        "np.save('node2vec_embeddings.npy', graph_embeddings)\n",
        "model_n2v.save('node2vec_model.bin')\n",
        "print(f\"\\nSaved embeddings and model\")"
      ],
      "metadata": {
        "id": "EwLcncKvM13D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm.notebook import tqdm\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATING STRUCTURAL FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "structural_features = {}\n",
        "feature_names = ['degree_centrality', 'clustering_coefficient', 'hits_authority']\n",
        "\n",
        "print(f\"\\nCalculating 3 graph-structural views for all nodes...\")\n",
        "\n",
        "deg_cent = nx.degree_centrality(G)\n",
        "clust_coeff = nx.clustering(G)\n",
        "\n",
        "try:\n",
        "    hubs, authorities = nx.hits(G, max_iter=100)\n",
        "    hits_auth = authorities\n",
        "except Exception as e:\n",
        "    print(f\"HITS failed: {e}. Falling back to PageRank for global score.\")\n",
        "    try:\n",
        "        hits_auth = nx.pagerank(G)\n",
        "    except Exception:\n",
        "        print(\"PageRank also failed. Assigning 0s.\")\n",
        "        hits_auth = {node: 0.0 for node in all_nodes['user_id']}\n",
        "\n",
        "for user_id in tqdm(all_nodes['user_id'], desc=\"Preparing structural sequences\"):\n",
        "    sequence = np.array([\n",
        "        [deg_cent.get(user_id, 0.0)],\n",
        "        [clust_coeff.get(user_id, 0.0)],\n",
        "        [hits_auth.get(user_id, 0.0)]\n",
        "    ])\n",
        "    structural_features[user_id] = sequence\n",
        "\n",
        "structural_array = np.array([structural_features[user_id] for user_id in all_nodes['user_id']])\n",
        "\n",
        "N, S, D = structural_array.shape\n",
        "temp_array = structural_array.reshape(-1, D)\n",
        "scaler_struct = StandardScaler()\n",
        "temp_array_norm = scaler_struct.fit_transform(temp_array)\n",
        "structural_array_norm = temp_array_norm.reshape(N, S, D)\n",
        "\n",
        "print(f\"\\nStructural sequences prepared: {len(structural_features):,} users\")\n",
        "print(f\"Sequence shape: {structural_array_norm.shape}\")\n",
        "\n",
        "temporal_sequences = structural_features\n",
        "temporal_array = structural_array_norm\n",
        "temporal_cols = feature_names"
      ],
      "metadata": {
        "id": "SbN2UPEdM6nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DEFINING MODEL ARCHITECTURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TemporalTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=128, nhead=8, num_layers=4,\n",
        "                 dim_feedforward=512, dropout=0.1):\n",
        "        super(TemporalTransformer, self).__init__()\n",
        "\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.output_projection = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_projection(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.output_projection(x)\n",
        "        return x\n",
        "\n",
        "class SpatialCNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_channels=[64, 128, 64],\n",
        "                 kernel_sizes=[3, 5, 7], output_dim=128):\n",
        "        super(SpatialCNN, self).__init__()\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        in_channels = 1\n",
        "\n",
        "        for hidden_ch, kernel_size in zip(hidden_channels, kernel_sizes):\n",
        "            self.convs.append(nn.Sequential(\n",
        "                nn.Conv1d(in_channels, hidden_ch, kernel_size, padding=kernel_size//2),\n",
        "                nn.BatchNorm1d(hidden_ch),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool1d(2),\n",
        "                nn.Dropout(0.3)\n",
        "            ))\n",
        "            in_channels = hidden_ch\n",
        "\n",
        "        self.flatten_dim = hidden_channels[-1] * (input_dim // (2 ** len(hidden_channels)))\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.flatten_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        for conv in self.convs:\n",
        "            x = conv(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class HybridInfluencerDetector(nn.Module):\n",
        "    def __init__(self, graph_embed_dim=128, static_feature_dim=6,\n",
        "                 temporal_feature_dim=1, d_model=128):\n",
        "        super(HybridInfluencerDetector, self).__init__()\n",
        "\n",
        "        self.temporal_transformer = TemporalTransformer(\n",
        "            input_dim=temporal_feature_dim,\n",
        "            d_model=d_model,\n",
        "            nhead=8,\n",
        "            num_layers=4\n",
        "        )\n",
        "\n",
        "        spatial_input_dim = graph_embed_dim + static_feature_dim\n",
        "        self.spatial_cnn = SpatialCNN(\n",
        "            input_dim=spatial_input_dim,\n",
        "            hidden_channels=[64, 128, 64],\n",
        "            output_dim=d_model\n",
        "        )\n",
        "\n",
        "        fusion_input_dim = d_model * 2\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(fusion_input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, graph_embeddings, static_features, temporal_sequences):\n",
        "        temporal_features = self.temporal_transformer(temporal_sequences)\n",
        "\n",
        "        spatial_input = torch.cat([graph_embeddings, static_features], dim=1)\n",
        "        spatial_features = self.spatial_cnn(spatial_input)\n",
        "\n",
        "        combined = torch.cat([temporal_features, spatial_features], dim=1)\n",
        "        output = self.fusion(combined)\n",
        "\n",
        "        return output\n",
        "\n",
        "print(\"Model architectures defined\")"
      ],
      "metadata": {
        "id": "UKTJcwyxNEQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PREPARING DATASETS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "feature_cols_for_model = [col for col in feature_cols_for_norm]\n",
        "\n",
        "static_features = all_nodes[feature_cols_for_model].values\n",
        "labels = all_nodes['is_hidden_influencer'].values\n",
        "\n",
        "temporal_array = np.array([temporal_sequences[user_id] for user_id in all_nodes['user_id']])\n",
        "\n",
        "print(f\"\\nData shapes:\")\n",
        "print(f\"Graph embeddings: {graph_embeddings.shape}\")\n",
        "print(f\"Static features: {static_features.shape}\")\n",
        "print(f\"Temporal sequences: {temporal_array.shape}\")\n",
        "print(f\"Labels: {labels.shape}\")\n",
        "\n",
        "indices = np.arange(len(all_nodes))\n",
        "train_idx, temp_idx = train_test_split(indices, test_size=0.3,\n",
        "                                       stratify=labels, random_state=42)\n",
        "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5,\n",
        "                                     stratify=labels[temp_idx], random_state=42)\n",
        "\n",
        "print(f\"\\nData split:\")\n",
        "print(f\"Train: {len(train_idx):,} ({len(train_idx)/len(all_nodes):.1%})\")\n",
        "print(f\"Val:   {len(val_idx):,} ({len(val_idx)/len(all_nodes):.1%})\")\n",
        "print(f\"Test:  {len(test_idx):,} ({len(test_idx)/len(all_nodes):.1%})\")\n",
        "\n",
        "class InfluencerDataset(Dataset):\n",
        "    def __init__(self, graph_emb, static_feat, temporal_seq, labels):\n",
        "        self.graph_emb = torch.FloatTensor(graph_emb)\n",
        "        self.static_feat = torch.FloatTensor(static_feat)\n",
        "        self.temporal_seq = torch.FloatTensor(temporal_seq)\n",
        "        self.labels = torch.FloatTensor(labels).unsqueeze(1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'graph_embeddings': self.graph_emb[idx],\n",
        "            'static_features': self.static_feat[idx],\n",
        "            'temporal_features': self.temporal_seq[idx],\n",
        "            'label': self.labels[idx]\n",
        "        }\n",
        "\n",
        "train_dataset = InfluencerDataset(\n",
        "    graph_embeddings[train_idx], static_features[train_idx], temporal_array[train_idx], labels[train_idx]\n",
        ")\n",
        "val_dataset = InfluencerDataset(\n",
        "    graph_embeddings[val_idx], static_features[val_idx], temporal_array[val_idx], labels[val_idx]\n",
        ")\n",
        "test_dataset = InfluencerDataset(\n",
        "    graph_embeddings[test_idx], static_features[test_idx], temporal_array[test_idx], labels[test_idx]\n",
        ")\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"\\nDataLoaders created:\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")"
      ],
      "metadata": {
        "id": "7zZG9ae7NH-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INITIALIZING MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "total_count = int(len(all_nodes))\n",
        "total_hidden_count = int(all_nodes['is_hidden_influencer'].sum())\n",
        "\n",
        "if total_hidden_count == 0:\n",
        "    print(\"\\nCRITICAL ERROR: Total hidden influencers is 0.\")\n",
        "    pos_weight_value = 1.0\n",
        "else:\n",
        "    pos_weight_value = (total_count - total_hidden_count) / total_hidden_count\n",
        "\n",
        "model = HybridInfluencerDetector(\n",
        "    graph_embed_dim=128,\n",
        "    static_feature_dim=len(feature_cols_for_model),\n",
        "    temporal_feature_dim=temporal_array.shape[2],\n",
        "    d_model=128\n",
        ").to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nModel statistics:\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "pos_weight_tensor = torch.tensor([pos_weight_value], dtype=torch.float).to(device)\n",
        "criterion = nn.BCELoss(weight=None, reduction='mean')\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', patience=5, factor=0.5\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining setup complete\")\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for data in tqdm(loader, desc=\"Training\"):\n",
        "        graph_emb = data['graph_embeddings'].to(device)\n",
        "        static_feat = data['static_features'].to(device)\n",
        "        temporal_feat = data['temporal_features'].to(device)\n",
        "        labels = data['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(graph_emb, static_feat, temporal_feat)\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * len(labels)\n",
        "\n",
        "        all_preds.extend(output.detach().cpu().numpy())\n",
        "        all_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_preds = np.array(all_preds)\n",
        "\n",
        "    try:\n",
        "        auc_score = roc_auc_score(all_labels, all_preds)\n",
        "    except ValueError:\n",
        "        auc_score = 0.5\n",
        "\n",
        "    return avg_loss, auc_score\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for data in tqdm(loader, desc=\"Validating\"):\n",
        "        graph_emb = data['graph_embeddings'].to(device)\n",
        "        static_feat = data['static_features'].to(device)\n",
        "        temporal_feat = data['temporal_features'].to(device)\n",
        "        labels = data['label'].to(device)\n",
        "\n",
        "        output = model(graph_emb, static_feat, temporal_feat)\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        total_loss += loss.item() * len(labels)\n",
        "\n",
        "        all_preds.extend(output.detach().cpu().numpy())\n",
        "        all_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_preds = np.array(all_preds)\n",
        "\n",
        "    try:\n",
        "        auc_score = roc_auc_score(all_labels, all_preds)\n",
        "        avg_precision = average_precision_score(all_labels, all_preds)\n",
        "    except ValueError:\n",
        "        auc_score = 0.5\n",
        "        avg_precision = 0.0\n",
        "\n",
        "    return avg_loss, auc_score, avg_precision\n",
        "\n",
        "epochs = 50\n",
        "best_val_auc = 0\n",
        "patience_counter = 0\n",
        "print(f\"\\nStarting training for {epochs} epochs...\")\n",
        "\n",
        "history = {'train_loss': [], 'train_auc': [], 'val_loss': [], 'val_auc': [], 'val_ap': []}\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss, train_auc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_auc, val_ap = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Train AUC: {train_auc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | Val AUC: {val_auc:.4f} | Val AP: {val_ap:.4f}\")\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_auc'].append(train_auc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_auc'].append(val_auc)\n",
        "    history['val_ap'].append(val_ap)\n",
        "\n",
        "    if val_auc > best_val_auc:\n",
        "        best_val_auc = val_auc\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "        print(\"Model saved\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= 10:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break\n",
        "\n",
        "print(\"\\nTraining complete\")"
      ],
      "metadata": {
        "id": "Ge2T4DfDNLsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import networkx as nx\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"HYBRID MODEL TESTING ON NEW DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "TEST_FILE_NODES = 'twitter_node_features.csv'\n",
        "TEST_FILE_EDGES = 'twitter_edges.csv'\n",
        "\n",
        "STATIC_FEATURE_NAMES = [\n",
        "    'followers',\n",
        "    'posts_count',\n",
        "    'engagement_rate',\n",
        "    'mfim_score',\n",
        "    'authenticity_score',\n",
        "    'pagerank'\n",
        "]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "best_model = HybridInfluencerDetector(\n",
        "    graph_embed_dim=128,\n",
        "    static_feature_dim=len(STATIC_FEATURE_NAMES),\n",
        "    temporal_feature_dim=1,\n",
        "    d_model=128\n",
        ").to(device)\n",
        "\n",
        "best_model.load_state_dict(torch.load('best_model.pt', map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "model_n2v = Word2Vec.load('node2vec_model.bin')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LOADING AND PREPROCESSING NEW DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if not os.path.exists(TEST_FILE_NODES) or not os.path.exists(TEST_FILE_EDGES):\n",
        "    raise FileNotFoundError(\"Required test files not found\")\n",
        "\n",
        "new_node_features = pd.read_csv(TEST_FILE_NODES)\n",
        "new_edge_list = pd.read_csv(TEST_FILE_EDGES)\n",
        "\n",
        "# -------------------------------\n",
        "# FIX NODE ID COLUMN\n",
        "# -------------------------------\n",
        "if 'user_id' not in new_node_features.columns:\n",
        "    if 'user' in new_node_features.columns:\n",
        "        new_node_features.rename(columns={'user': 'user_id'}, inplace=True)\n",
        "    elif 'channel_name' in new_node_features.columns:\n",
        "        new_node_features.rename(columns={'channel_name': 'user_id'}, inplace=True)\n",
        "    else:\n",
        "        raise KeyError(\"No valid node ID column found\")\n",
        "\n",
        "node_ids_new = new_node_features['user_id'].unique()\n",
        "new_data = new_node_features.set_index('user_id')\n",
        "\n",
        "# -------------------------------\n",
        "# FIX EDGE COLUMN NAMES\n",
        "# -------------------------------\n",
        "if 'source_user_id' not in new_edge_list.columns:\n",
        "    if 'source' in new_edge_list.columns and 'target' in new_edge_list.columns:\n",
        "        new_edge_list.rename(\n",
        "            columns={\n",
        "                'source': 'source_user_id',\n",
        "                'target': 'target_user_id'\n",
        "            },\n",
        "            inplace=True\n",
        "        )\n",
        "    else:\n",
        "        raise KeyError(\"No valid edge columns found\")\n",
        "\n",
        "# -------------------------------\n",
        "# FILL MISSING STATIC FEATURES\n",
        "# -------------------------------\n",
        "for col in STATIC_FEATURE_NAMES:\n",
        "    if col not in new_data.columns:\n",
        "        new_data[col] = 0.0\n",
        "\n",
        "X_static_new = new_data[STATIC_FEATURE_NAMES].values\n",
        "\n",
        "# -------------------------------\n",
        "# NODE2VEC EMBEDDINGS\n",
        "# -------------------------------\n",
        "graph_embeddings_new = []\n",
        "for user_id in tqdm(node_ids_new):\n",
        "    try:\n",
        "        emb = model_n2v.wv[user_id]\n",
        "    except KeyError:\n",
        "        emb = np.zeros(128)\n",
        "    graph_embeddings_new.append(emb)\n",
        "\n",
        "graph_embeddings_new = np.array(graph_embeddings_new)\n",
        "\n",
        "# -------------------------------\n",
        "# BUILD GRAPH FOR STRUCTURAL FEATURES\n",
        "# -------------------------------\n",
        "G_new = nx.Graph()\n",
        "for _, row in new_edge_list.iterrows():\n",
        "    G_new.add_edge(row['source_user_id'], row['target_user_id'])\n",
        "\n",
        "deg_cent = nx.degree_centrality(G_new)\n",
        "clust_coeff = nx.clustering(G_new)\n",
        "\n",
        "try:\n",
        "    _, authorities = nx.hits(G_new, max_iter=100)\n",
        "    hits_auth = authorities\n",
        "except Exception:\n",
        "    hits_auth = nx.pagerank(G_new)\n",
        "\n",
        "temporal_array_new = []\n",
        "for user_id in node_ids_new:\n",
        "    temporal_array_new.append([\n",
        "        [deg_cent.get(user_id, 0.0)],\n",
        "        [clust_coeff.get(user_id, 0.0)],\n",
        "        [hits_auth.get(user_id, 0.0)]\n",
        "    ])\n",
        "\n",
        "temporal_array_new = np.array(temporal_array_new)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RUNNING INFERENCE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "graph_emb_tensor = torch.FloatTensor(graph_embeddings_new).to(device)\n",
        "static_feat_tensor = torch.FloatTensor(X_static_new).to(device)\n",
        "temporal_feat_tensor = torch.FloatTensor(temporal_array_new).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions_prob = best_model(\n",
        "        graph_emb_tensor,\n",
        "        static_feat_tensor,\n",
        "        temporal_feat_tensor\n",
        "    ).cpu().numpy().flatten()\n",
        "\n",
        "predictions = (predictions_prob >= 0.5).astype(int)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAVING RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'user_id': node_ids_new,\n",
        "    'predicted_label': predictions,\n",
        "    'prediction_probability': predictions_prob\n",
        "})\n",
        "\n",
        "results_df = results_df.merge(new_node_features, on='user_id', how='left')\n",
        "results_df.to_csv('predictions_out_of_sample.csv', index=False)\n",
        "\n",
        "print(\"Inference complete\")\n",
        "print(f\"Total Nodes: {len(node_ids_new)}\")\n",
        "print(f\"Predicted Influencers: {int(predictions.sum())}\")\n",
        "print(f\"Prediction Rate: {predictions.mean():.2%}\")\n"
      ],
      "metadata": {
        "id": "bRScgC0gNR5O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyuCRa6ZTrVuBJKCjLyn8o",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}